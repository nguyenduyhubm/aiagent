https://huggingface.co/hiieu/halong_embedding
pip install -U sentence-transformers

 from sentence_transformers import SentenceTransformer
import torch

# Download from the ü§ó Hub
model = SentenceTransformer("hiieu/halong_embedding")

# Define query and documents
#query = "B√≥ng ƒë√° c√≥ l·ª£i √≠ch g√¨ cho s·ª©c kh·ªèe?"
query = "Tuy·∫øn quang 10G ƒë∆∞·ª£c ph√©p x·ª≠ l√Ω trong bao l√¢u?"
docs = [
    "B√≥ng ƒë√° gi√∫p c·∫£i thi·ªán s·ª©c kh·ªèe tim m·∫°ch v√† tƒÉng c∆∞·ªùng s·ª©c b·ªÅn.",
    "Quy ƒë·ªãnh x·ª≠ l√Ω tuy·∫øn quang 10G l√† 10 gi·ªù.",
        "Quy ƒë·ªãnh x·ª≠ l√Ω tuy·∫øn quang 1G l√† 1 gi·ªù.",
    "B√≥ng ƒë√° l√† m√¥n th·ªÉ thao ph·ªï bi·∫øn nh·∫•t th·∫ø gi·ªõi.",
    "Ch∆°i b√≥ng ƒë√° gi√∫p gi·∫£m cƒÉng th·∫≥ng v√† c·∫£i thi·ªán t√¢m l√Ω.",
    "B√≥ng ƒë√° c√≥ th·ªÉ gi√∫p b·∫°n k·∫øt n·ªëi v·ªõi nhi·ªÅu ng∆∞·ªùi h∆°n.",
    "B√≥ng ƒë√° kh√¥ng ch·ªâ l√† m√¥n th·ªÉ thao m√† c√≤n l√† c√°ch ƒë·ªÉ gi·∫£i tr√≠."
]

# Encode query and documents
query_embedding = model.encode([query])
doc_embeddings = model.encode(docs)
similarities = model.similarity(query_embedding, doc_embeddings).flatten()

# Sort documents by cosine similarity
sorted_indices = torch.argsort(similarities, descending=True)
sorted_docs = [docs[idx] for idx in sorted_indices]
sorted_scores = [similarities[idx].item() for idx in sorted_indices]

# Print sorted documents with their cosine scores
for doc, score in zip(sorted_docs, sorted_scores):
    print(f"Document: {doc} - Cosine Similarity: {score:.4f}")

##K·∫øt qu·∫£:
##Document: B√≥ng ƒë√° gi√∫p c·∫£i thi·ªán s·ª©c kh·ªèe tim m·∫°ch v√† tƒÉng c∆∞·ªùng s·ª©c b·ªÅn. - Cosine Similarity: 0.7318
##Document: Ch∆°i b√≥ng ƒë√° gi√∫p gi·∫£m cƒÉng th·∫≥ng v√† c·∫£i thi·ªán t√¢m l√Ω. - Cosine Similarity: 0.6623
##Document: B√≥ng ƒë√° kh√¥ng ch·ªâ l√† m√¥n th·ªÉ thao m√† c√≤n l√† c√°ch ƒë·ªÉ gi·∫£i tr√≠. - Cosine Similarity: 0.6102
##Document: B√≥ng ƒë√° c√≥ th·ªÉ gi√∫p b·∫°n k·∫øt n·ªëi v·ªõi nhi·ªÅu ng∆∞·ªùi h∆°n. - Cosine Similarity: 0.4988
##Document: B√≥ng ƒë√° l√† m√¥n th·ªÉ thao ph·ªï bi·∫øn nh·∫•t th·∫ø gi·ªõi. - Cosine Similarity: 0.4828
##
##================== RESTART: D:/dotnet/2024/AI/rag_embedding.py =================
##Document: Quy ƒë·ªãnh x·ª≠ l√Ω tuy·∫øn quang 10G l√† 10 gi·ªù. - Cosine Similarity: 0.8367
##Document: Quy ƒë·ªãnh x·ª≠ l√Ω tuy·∫øn quang 1G l√† 1 gi·ªù. - Cosine Similarity: 0.6227
##Document: B√≥ng ƒë√° c√≥ th·ªÉ gi√∫p b·∫°n k·∫øt n·ªëi v·ªõi nhi·ªÅu ng∆∞·ªùi h∆°n. - Cosine Similarity: 0.0438
##Document: B√≥ng ƒë√° gi√∫p c·∫£i thi·ªán s·ª©c kh·ªèe tim m·∫°ch v√† tƒÉng c∆∞·ªùng s·ª©c b·ªÅn. - Cosine Similarity: 0.0122
##Document: Ch∆°i b√≥ng ƒë√° gi√∫p gi·∫£m cƒÉng th·∫≥ng v√† c·∫£i thi·ªán t√¢m l√Ω. - Cosine Similarity: 0.0019
##Document: B√≥ng ƒë√° kh√¥ng ch·ªâ l√† m√¥n th·ªÉ thao m√† c√≤n l√† c√°ch ƒë·ªÉ gi·∫£i tr√≠. - Cosine Similarity: -0.0158
##Document: B√≥ng ƒë√° l√† m√¥n th·ªÉ thao ph·ªï bi·∫øn nh·∫•t th·∫ø gi·ªõi. - Cosine Similarity: -0.0429

